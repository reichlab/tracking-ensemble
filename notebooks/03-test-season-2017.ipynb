{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Test models on the live season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1234)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from utils import data as udata\n",
    "from utils import dists as udists\n",
    "from utils import misc as u\n",
    "from tqdm import tqdm, trange\n",
    "import os.path as path\n",
    "import json\n",
    "import stringcase\n",
    "import models\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = \"../data/processed/cdc-flusight-ensemble-live/\"\n",
    "INPUT_DIR = \"../models/cdc-flusight-ensemble/\"\n",
    "TARGET = \"1-ahead\"\n",
    "# We use all the regions as of now\n",
    "REGION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [udata.Component(EXP_DIR, m) for m in udata.available_models(EXP_DIR)]\n",
    "ad = udata.ActualData(EXP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(model):\n",
    "    \"\"\"\n",
    "    Return score and model predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    yi, Xs, y = udata.get_seasons_data(ad, components, [None], TARGET, REGION)\n",
    "    predictions = np.zeros_like(Xs[0])\n",
    "\n",
    "    for i in range(len(yi)):\n",
    "        # HACK: Check if this is an oracle\n",
    "        # This should ideally go in as a flag in the model\n",
    "        if \"truth\" in inspect.signature(model.predict).parameters:\n",
    "            # This is an oracle\n",
    "            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs], y[i])\n",
    "        else:\n",
    "            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs]) \n",
    "        # Pass in feedback if model accepts it\n",
    "        try:\n",
    "            model.feedback(y[i])\n",
    "        except NotImplementedError:\n",
    "            pass\n",
    "\n",
    "    score = np.log(udists.prediction_probabilities([predictions], y, TARGET)).mean()\n",
    "    return score, predictions\n",
    "\n",
    "def evaluate(model, post_training_hook=None):\n",
    "    \"\"\"\n",
    "    Evaluate on the testing seasons\n",
    "    \"\"\"\n",
    "    \n",
    "    # Need to reset the model before every evaluation\n",
    "    # TODO: This should be done even when not using a hook\n",
    "    if post_training_hook:\n",
    "        model = post_training_hook(model)\n",
    "    score, _= _evaluate(model)\n",
    "    return score\n",
    "\n",
    "def load_model(model):\n",
    "    \"\"\"\n",
    "    Load weights from saved\n",
    "    \"\"\"\n",
    "    \n",
    "    model_file_name = f\"{stringcase.spinalcase(type(model).__name__)}.json\"\n",
    "    model.load(path.join(INPUT_DIR, TARGET, model_file_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle model\n",
    "This is the oracle model which sets the upper limit for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2603335204015305"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(models.OracleEnsemble(TARGET, len(components)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.326715479411095"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(models.MeanEnsemble(TARGET, len(components)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degenerate EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.2495550469619876"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.DemWeightEnsemble(TARGET, len(components)))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-partition Degenerate EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.2495550469619876"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.KDemWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit weight ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.23448551225504"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.HitWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score weight ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.207411559712781"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.ScoreWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.246089721420991"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a post training hook\n",
    "def pth(model):\n",
    "    # Read initial weights from DEM\n",
    "    with open(path.join(INPUT_DIR, TARGET, \"dem-weight-ensemble.json\")) as fp:\n",
    "        model._weights = json.load(fp)[\"fit_params\"][\"weights\"]\n",
    "    return model\n",
    "\n",
    "def pth_reset(model):\n",
    "    model._weights = np.ones((model.n_comps,)) / model.n_comps\n",
    "    return model\n",
    "\n",
    "m = load_model(models.MPWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m, pth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
