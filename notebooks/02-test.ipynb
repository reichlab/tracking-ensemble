{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Test models on the [2014, 2016] set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1234)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from utils import data as udata\n",
    "from utils import dists as udists\n",
    "from utils import misc as u\n",
    "from tqdm import tqdm, trange\n",
    "import os.path as path\n",
    "import json\n",
    "import stringcase\n",
    "import models\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_DIR = \"../data/processed/cdc-flusight-ensemble/\"\n",
    "INPUT_DIR = \"../models/cdc-flusight-ensemble/\"\n",
    "TARGET = \"1-ahead\"\n",
    "# We use all the regions as of now\n",
    "REGION = None\n",
    "# We have training data from the set [2010, 2016]. From these, we use\n",
    "# [2010, 2013] for training and CV.\n",
    "TESTING_SEASONS = list(range(2014, 2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [udata.Component(EXP_DIR, m) for m in udata.available_models(EXP_DIR)]\n",
    "ad = udata.ActualData(EXP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate the models on the left out season and take mean across all such evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_season(model, season):\n",
    "    \"\"\"\n",
    "    Return score and model predictions for given season\n",
    "    \"\"\"\n",
    "    \n",
    "    yi, Xs, y = udata.get_seasons_data(ad, components, [season], TARGET, REGION)\n",
    "    predictions = np.zeros_like(Xs[0])\n",
    "\n",
    "    for i in range(len(yi)):\n",
    "        # HACK: Check if this is an oracle\n",
    "        # This should ideally go in as a flag in the model\n",
    "        if \"truth\" in inspect.signature(model.predict).parameters:\n",
    "            # This is an oracle\n",
    "            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs], y[i])\n",
    "        else:\n",
    "            predictions[i, :] = model.predict(yi.iloc[i], [X[i] for X in Xs]) \n",
    "        # Pass in feedback if model accepts it\n",
    "        try:\n",
    "            model.feedback(y[i])\n",
    "        except NotImplementedError:\n",
    "            pass\n",
    "\n",
    "    score = np.log(udists.prediction_probabilities([predictions], y, TARGET)).mean()\n",
    "    return score, predictions\n",
    "\n",
    "def evaluate(model, post_training_hook=None):\n",
    "    \"\"\"\n",
    "    Evaluate on the testing seasons\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    for season in tqdm(TESTING_SEASONS):\n",
    "        # Need to reset the model before every evaluation\n",
    "        # TODO: This should be done even when not using a hook\n",
    "        if post_training_hook:\n",
    "            model = post_training_hook(model)\n",
    "        score, _= evaluate_season(model, season)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def load_model(model):\n",
    "    \"\"\"\n",
    "    Load weights from saved\n",
    "    \"\"\"\n",
    "    \n",
    "    model_file_name = f\"{stringcase.spinalcase(type(model).__name__)}.json\"\n",
    "    model.load(path.join(INPUT_DIR, TARGET, model_file_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle model\n",
    "This is the oracle model which sets the upper limit for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:26<00:00,  8.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.8314510639294843"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(models.OracleEnsemble(TARGET, len(components)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.782174668037595"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(models.MeanEnsemble(TARGET, len(components)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degenerate EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.618871198618432"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.DemWeightEnsemble(TARGET, len(components)))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-partition Degenerate EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.618871198618432"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.KDemWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit weight ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.6544628191807207"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.HitWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score weight ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.6252054613192026"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = load_model(models.ScoreWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.6181334017298283"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a post training hook\n",
    "def pth(model):\n",
    "    # Read initial weights from DEM\n",
    "    with open(path.join(INPUT_DIR, TARGET, \"dem-weight-ensemble.json\")) as fp:\n",
    "        model._weights = json.load(fp)[\"fit_params\"][\"weights\"]\n",
    "    return model\n",
    "\n",
    "def pth_reset(model):\n",
    "    model._weights = np.ones((model.n_comps,)) / model.n_comps\n",
    "    return model\n",
    "\n",
    "m = load_model(models.MPWeightEnsemble(TARGET, len(components), None))\n",
    "evaluate(m, pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
